{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"argilla/tripadvisor-hotel-reviews\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding with the pad token\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
    "    adafactor=False,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    auto_find_batch_size=False,\n",
    "    batch_eval_metrics=False,\n",
    "    bf16=False,\n",
    "    bf16_full_eval=False,\n",
    "    data_seed=None,\n",
    "    dataloader_drop_last=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_persistent_workers=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_prefetch_factor=None,\n",
    "    ddp_backend=None,\n",
    "    ddp_broadcast_buffers=None,\n",
    "    ddp_bucket_cap_mb=None,\n",
    "    ddp_find_unused_parameters=None,\n",
    "    ddp_timeout=1800,\n",
    "    debug=[],\n",
    "    deepspeed=None,\n",
    "    disable_tqdm=True,\n",
    "    dispatch_batches=None,\n",
    "    do_eval=True,\n",
    "    do_predict=False,\n",
    "    do_train=False,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_delay=0,\n",
    "    eval_do_concat_batches=True,\n",
    "    eval_on_start=False,\n",
    "    eval_steps=None,\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_use_gather_object=False,\n",
    "    fp16=False,\n",
    "    fp16_backend=\"auto\",\n",
    "    fp16_full_eval=False,\n",
    "    fp16_opt_level=\"O1\",\n",
    "    fsdp=[],\n",
    "    fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
    "    fsdp_min_num_params=0,\n",
    "    fsdp_transformer_layer_cls_to_wrap=None,\n",
    "    full_determinism=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs=None,\n",
    "    greater_is_better=None,\n",
    "    group_by_length=False,\n",
    "    half_precision_backend=\"auto\",\n",
    "    hub_always_push=False,\n",
    "    hub_model_id=None,\n",
    "    hub_private_repo=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_token=\"<HUB_TOKEN>\",\n",
    "    ignore_data_skip=False,\n",
    "    include_inputs_for_metrics=False,\n",
    "    include_num_input_tokens_seen=False,\n",
    "    include_tokens_per_second=False,\n",
    "    jit_mode_eval=False,\n",
    "    label_names=None,\n",
    "    label_smoothing_factor=0.0,\n",
    "    learning_rate=5e-05,\n",
    "    length_column_name=\"length\",\n",
    "    load_best_model_at_end=False,\n",
    "    local_rank=0,\n",
    "    log_level=\"passive\",\n",
    "    log_level_replica=\"warning\",\n",
    "    log_on_each_node=True,\n",
    "    logging_dir=\"output_dir/runs/Jan01_15-17-36_fa8372f5-394b-4363-9cc4-dd02a34e7b18\",\n",
    "    logging_first_step=False,\n",
    "    logging_nan_inf_filter=True,\n",
    "    logging_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_kwargs={},\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_grad_norm=1.0,\n",
    "    max_steps=-1,\n",
    "    metric_for_best_model=None,\n",
    "    mp_parameters=\"\",\n",
    "    neftune_noise_alpha=None,\n",
    "    no_cuda=False,\n",
    "    num_train_epochs=3.0,\n",
    "    optim=\"adamw_torch\",\n",
    "    optim_args=None,\n",
    "    optim_target_modules=None,\n",
    "    output_dir=\"output_dir\",\n",
    "    overwrite_output_dir=False,\n",
    "    past_index=-1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    prediction_loss_only=False,\n",
    "    push_to_hub=False,\n",
    "    push_to_hub_model_id=None,\n",
    "    push_to_hub_organization=None,\n",
    "    ray_scope=\"last\",\n",
    "    remove_unused_columns=True,\n",
    "    report_to=['tensorboard'],\n",
    "    restore_callback_states_from_checkpoint=False,\n",
    "    resume_from_checkpoint=None,\n",
    "    run_name=\"output_dir\",\n",
    "    save_on_each_node=False,\n",
    "    save_only_model=False,\n",
    "    save_safetensors=True,\n",
    "    save_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    "    skip_memory_metrics=True,\n",
    "    split_batches=None,\n",
    "    tf32=None,\n",
    "    torch_compile=False,\n",
    "    torch_compile_backend=None,\n",
    "    torch_compile_mode=None,\n",
    "    torch_empty_cache_steps=None,\n",
    "    torchdynamo=None,\n",
    "    tpu_metrics_debug=False,\n",
    "    tpu_num_cores=None,\n",
    "    use_cpu=False,\n",
    "    use_ipex=False,\n",
    "    use_legacy_prediction_loop=False,\n",
    "    use_liger_kernel=False,\n",
    "    use_mps_device=False,\n",
    "    warmup_ratio=0.0,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_datasets[\"train\"])\n",
    "\n",
    "tokenized_datasets[\"train\"], tokenized_datasets[\"validation\"] = tokenized_datasets[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTForSequenceClassification were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, OpenAIGPTForSequenceClassification\n",
    "\n",
    "\n",
    "model = OpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "training_dataset = tokenized_datasets[\"train\"]\n",
    "testing_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=training_dataset, eval_dataset=testing_dataset,\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828e7b1049894c97bb4a9bc12d6a190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547b8fa91332464c9e514507a95a20fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/39.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6e5a5255f44e15814db6b4d49bf4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/2.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a975c3c4e1440e99e3c891efdce79ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/43835 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f526191629445769fa98c902e37968f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b298f1760a6d4787bb3645456af45e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43835 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi, I want to learn to play horseshoes. Can you teach me?']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "preference_data = load_dataset(\"trl-internal-testing/hh-rlhf-helpful-base-trl-style\", split=\"train\")\n",
    "\n",
    "# Define a function to extract the prompt\n",
    "def extract_prompt(text):\n",
    "    prompt = text[0]['content']\n",
    "    return prompt\n",
    "\n",
    "# Apply the function to the dataset \n",
    "preference_data_with_prompt = preference_data.map(\n",
    "    lambda sample: {**sample, 'prompt': extract_prompt(sample['chosen'])}\n",
    ")\n",
    "\n",
    "sample = preference_data_with_prompt.select(range(1))\n",
    "print(sample['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'content': 'Hi, I want to learn to play horseshoes. Can you teach me?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'I can, but maybe I should begin by telling you that a typical game consists of 2 players and 6 or 8 horseshoes.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Okay. What else is needed to play, and what are the rules?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Horseshoes are either metal or plastic discs. The horseshoes come in different weights, and the lighter ones are easier to throw, so they are often the standard for beginning players.',\n",
       "   'role': 'assistant'}]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['prompt']\n",
    "sample['chosen']\n",
    "sample['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
